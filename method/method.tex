\chapter{Diversification with Unison}

As mentioned briefly in Section \ref{sec:constraint}, the only constraint solver currently
supported by Unison is Gecode. While Unison in it's entirety is more than the solver it is
exclusively in the solver our diversification process takes place.

A branch and bound search can be implemented in Gecode by overriding a virtual member
function. This function take the most recently found solution as an argument and posts
constraints on the next solution based on the values of the argument. These constraints
accumulate so that all future solutions will be affected by all already found solutions.

Each strategy is implemented by constraints that, when satisfied, ensures the
future solutions differ in the sought after manner. These constraints are posted in this
virtual function. In general the constraints will make sure that future solutions does not
contain the same combination of variable - value mapping for certain variables.

Due to the nature of Unison and the Gecode constraint solver calling the implementations
\textit{transformations}, which is the common nomenclature in software diversity, would be
misleading. No code is being explicitly transformed, combinations are instead implicitly
discarded when it is discovered that the constraints cannot be satisfied given a previous
branching.

The implementation that makes up the following strategies that are used in the experiment
is henceforth referred to as disUnison (from the word disunity). It is a variation of the
original Unison model, where the key difference is that the behaviour of the branch and
bound search is modified. The bulk of the Unison model, that ensures functional code is
emitted, is still present in the disUnison model.

\input{method/strategies.tex}

\section{Number of Possible Versions}

The number of possible combinations is of course not limitless. 1000 version at a sampling
rate of 1000 means that there needs to be at least 1000000 possible solutions. Unison works
at the function level and for every function there might not be 1000000 possible versions.

Total number of possible combination would be an interesting metric to evaluate, unfortunately
it varies widely between functions and for some it might require days of search. Empirically,
most functions in the suite to be used do have 1000000 versions, so 1000 versions appears
to be a good number of versions to generate.

For those function where the 1000 versions cannot be generated for the given strategy and
sampling rate the ones that have been generated will be re-used so that 1000 programs can
still be generated. More information about these functions can be seen in the
\ref{appendix:function_names}.

\section{Performance}
\label{sec:performance}

An important variable in Unison's original model is cost. It is the deciding factor of
whether a solution is better than another during the branch and bound search. It is a sum
of the estimated cost of each basic block, weighted by the estimated execution frequencies
\cite{unison-docs}. Cost can be either cycles of file size depending on the optimization
goal. For the experiment the optimization goal will be speed, and thus cost will be
measured in cycles.

In the original Unison model the cost variable is used both for branching and during the
branch and bound process. In the disUnison model it is still used for branching, but future
solutions are not bound to have lower cost. When used for branching lower values of cost
are explored before higher ones, and thus lower cost solutions are found before higher
cost solutions.

Unison (and disUnison) accepts the basic LLVM-solution as an optional parameter, and can
post constraints to only generate solutions that are \textit{better}. In other words, we
can generate executables with zero overhead \textit{with respect to LLVM's solution}.
Certain strategies would of course have an overhead, but with respect to the
\textit{optimal solution}. As this would limit the number of possible version this
optional parameter will not be used during the experiment. It is however an exciting
factor to consider for future work.

\section{Sampling Rate}
\label{sec:sampling_rate}

When exploring combinations with a constraint solver similar solutions are found close to
each-other (in-time). A factor in diversification might be to exploit this property
alongside the diversification strategy. Certain strategies might be favorable when
considering execution time but not particularly good at breaking gadgets. However, if
e.g 100 solutions are discarded between every emitted executable perhaps more gadgets are
broken.

An important factor to consider when discarding solutions is that in the disUnison model
branching explores lower cost solutions first (see \ref{sec:performance}. Discarding
solutions can thus impact performance in a negative way in the sense that the later
version might have a wider distribution for the cost variable.

Sampling rates of 1, 10, 100 and 1000 will be evaluated for every strategy, where a
sampling rate of 100 means that every 100th solution is kept. Generating 1000 versions
at a sampling rate of 100 would mean that 100000 solutions are explored, 1000 are emitted
and 99000 are discarded.

\section{Architecture}
\label{sec:arch}

Unison does not currently support the x86 or x86-64 architecture. Only ARM, Hexagon and MIPS
are supported \cite{unison-src}. None of the supported architectures are generally considered
when testing automated software diversity, but for the purposes of evaluating the use of
a systematic approach the supported architectures offers a glimpse at the potential. For
the experiment the code will be compiled for the Hexagon architecture.

Hexagon functions very differently from x86 but for our purposes targeting Hexagon will
still hint at the gadget-breaking potential of the systematic approach. After all,
breaking gadgets is about shifting, adding, removing or otherwise modifying \textit{any}
instruction in the program, and since the diversification strategies are applied globally
they are supposedly equally effective regardless of the placement or structure of the
instruction.

\section{Experiment}

\subsection{Metrics}

The key metric to be evaluated are surviving gadgets, \textit{cost} (See
\ref{sec:performance}) and the execution time of the code generator. For the experiment
the optimization goal will be speed, and thus cost will be the estimated execution time of
the program (in cycles).

Surviving gadgets will be calculated as the ratio of which each gadget appears in the program
versions. In other words a ratio of 100\% means that the gadget appears in all version
and the strategy was not effective at removing that gadget. Similarly a very low ratio
(close to 0\%) would mean that the strategy was effective as the gadget only appears in
a small number of the programs.

\subsection{Process}

\textcite{large-scale-automated} found 433.milc from the SPEC2006 benchmark suite to be
representative in terms of surviving gadgets. Unfortunately 433.milc is too large to run
any feasible tests on my test machine, which runs a 4-core Intel(R) Core(TM) i7-4500U CPU
@ 1.80GHz and 8 gigabytes of memory. The Unison test suite will be used instead, which is
a sample of functions from the SPEC2006 benchmarks. There are 23 functions and their names
are listed in \ref{appendix:function_names}. These functions will together make a
\textit{program}. Since they do not make up a complete executable they cannot be linked
together. Instead linking will be simulated by placing them in the same order every time
to ensure a fair comparison between them.

Our method of testing is thus as follows:

\begin{enumerate}
	\item For every function generate 1000 versions
	\item Version 0 of every function will make up program 0, version 1 will make up program
		1 and so forth
	\item For every program find all gadgets and its cost (in cycles)
	\item Calculate our metrics
	\item Repeat for all strategies
		\begin{itemize}
			\item enumerate
			\item registers
			\item schedule
		\end{itemize}
	\item Repeat for all sampling rates
		\begin{itemize}
			\item 1
			\item 10
			\item 100
			\item 1000
		\end{itemize}
\end{enumerate}

In other words, for every combination of strategy and sampling rate (12 in total) 1000
different versions of all 23 functions will be generated. These function versions will
then be combined to yield 1000 different program versions.
