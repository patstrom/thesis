\chapter{Diversification with Unison}

\section{Automatic Diversity Synthesis with Unison}
\label{sec:unison-model}

% Write how Unison works a single function and llc (llvm code generator) should link these
% together.

As described in Section \ref{sec:unison} the problem of integrated register allocation and
instruction scheduling in Unison is modeled around \textit{operations} and \textit{operands}.
The problem consists of around 20 variables (see \ref{sec:constraint}) in total so only the
ones relevant for the experiment will be presented. These variables describe how the
operations and their operands relate to the actual instructions, temporaries, registers
and issue cycles.

The only constraint solver currently supported by Unison is Gecode\cite{unison-docs}. A
branch and bound search can be implemented in Gecode by using a branch-and-bound search
engine and overriding the virtual member function \textit{constrain()} of the model. The
\textit{constrain()} function is invoked by the search engine whenever a solution is found
and takes the most recently found solution as an argument. The idea is to post constraints
on the next solution based on the values of the argument. These constraints accumulate so
that all future solutions will be affected by all already found solutions.

An important variable in the Unison model is \texit{cost}. It is the deciding factor
of whether a solution is better than another during the branch and bound search. It is a
sum of the estimated cost of each basic block, weighted by the estimated execution
frequencies \cite{unison-docs}. Cost can be either cycles or code size depending on the
optimization goal. I.e the \textit{constrain()} function of the Unison model post the
constraint that for future combinations to be considered solutions, in addition to the
original constraints, the cost must be less than the cost of the previously found solution.
Given enough execution time, Unison will thus find the optimal solution with regards to
the \textit{cost} variable.

As mentioned, our intention is to replace the current \textit{contrain()} function of
Unison with one that instead posts constraints to ensure that future solutions are
\textit{different} in some regard.

% Do I want to mention branching and such?

% Revise(?)
%Due to the nature of Unison and the Gecode constraint solver calling the implementations
%\textit{transformations}, which is the common nomenclature in software diversity, would be
%misleading. No code is being explicitly transformed, combinations are instead implicitly
%discarded when it is discovered that the constraints cannot be satisfied given a previous
%branching.

\section{disUnison}

The implementation that makes up the following strategies that are used in the experiment
is henceforth referred to as disUnison (from the word disunity). It is a variation of the
original Unison model, where the key difference is that the behaviour of the branch and
bound search is modified. The bulk of the Unison model, that ensures functional code is
emitted, is still present in the disUnison model.

\input{method/strategies.tex}

\section{Sampling Rate}
\label{sec:sampling_rate}

When exploring combinations with a constraint solver similar solutions are found close to
each-other (in-time). A factor in diversification might be to exploit this property
alongside the diversification strategy. Certain strategies might be favorable when
considering execution time but not particularly good at breaking gadgets. However, if
e.g 100 solutions are discarded between every emitted executable perhaps more gadgets are
broken.

An important factor to consider when discarding solutions is that in the disUnison model
branching explores lower cost solutions first (see Section \ref{sec:unison-model,sec:performance}).
Discarding solutions can thus impact performance in a negative way in the sense that the
later versions might have a higher cost, resulting in a wider cost distribution across all
program versions.

Sampling rates of 1, 10, 100 and 1000 will be evaluated for every strategy, where a
sampling rate of 100 means that every 100th solution is kept. Generating 1000 versions
at a sampling rate of 100 would mean that 100000 solutions are explored, 1000 are emitted
and 99000 are discarded.

The number of possible combinations is of course not limitless. 1000 version at a sampling
rate of 1000 means that there needs to be at least 1000000 possible solutions. Unison works
at the function level and for every function there might not be 1000000 possible versions.

Total number of possible combination would be an interesting metric to evaluate, unfortunately
it varies widely between functions and for some it might require days of search. Empirically,
most functions in the suite to be used do have 1000000 versions, so 1000 versions appears
to be a good number of versions to generate.

For those function where the 1000 versions cannot be generated for the given strategy and
sampling rate the ones that have been generated will be re-used so that 1000 programs can
still be generated. More information about these functions can be seen in appendix
\ref{appendix:function_names}.

\subsection{Performance}
\label{sec:performance}

% This should/could be in the previous subsection
In the original Unison model the cost variable is used both for branching and during the
branch and bound process. In the disUnison model it is still used for branching, but future
solutions are not bound to have lower cost. When used for branching lower values of cost
are explored before higher ones, and thus lower cost solutions are found before higher
cost solutions.

% This should probably be in the Unison section
Unison (and disUnison) accepts the basic LLVM-solution as an optional parameter, and can
post constraints to only generate solutions that are \textit{better}. In other words, we
can generate executables with zero overhead \textit{with respect to LLVM's solution}.
Certain strategies would of course have an overhead, but with respect to the
\textit{optimal solution}. As this would limit the number of possible version this
optional parameter will not be used during the experiment. It is however an exciting
factor to consider for future work.

\subsection{Architecture}
\label{sec:arch}

Unison does not currently support the x86 or x86-64 architecture. Only ARM, Hexagon and MIPS
are supported \cite{unison-src}. None of the supported architectures are generally considered
when testing automated software diversity, but for the purposes of evaluating the use of
a systematic approach the supported architectures offers a glimpse at the potential. For
the experiment the code will be compiled for the Hexagon architecture.

Hexagon functions very differently from x86 but for our purposes targeting Hexagon will
still hint at the gadget-breaking potential of the systematic approach. After all,
breaking gadgets is about shifting, adding, removing or otherwise modifying \textit{any}
instruction in the program, and since the diversification strategies are applied globally
they are supposedly equally effective regardless of the placement or structure of the
instruction.

% Experimental Protocol (kasst namn)
\section{Experimental Protocol}

% TODO: Intro to section
% TODO: Subsection of dataset

\subsection{Metrics}

% Why are these key? Are they key? Rephrase paragraph
The key metric to be evaluated are surviving gadgets, \textit{cost} (See
\ref{sec:performance}) and the execution time of the code generator. For the experiment
the optimization goal will be speed, and thus cost will be the estimated execution time of
the program (in cycles).

% Make a more formal definition of the metric. It is calculated on a population of
% programs and I enumerate gadgets present in the population.
Surviving gadgets will be calculated as the ratio of which each gadget appears in the program
versions. In other words a ratio of 100\% means that the gadget appears in all version
and the strategy was not effective at removing that gadget. Similarly a very low ratio
(close to 0\%) would mean that the strategy was effective as the gadget only appears in
a small number of the programs.

\subsection{Data Set}

\subsection{Process}

% Don't talk about what I Don't do here. This should be in the discussion.
\textcite{large-scale-automated} found 433.milc from the SPEC2006 benchmark suite to be
representative in terms of surviving gadgets. Unfortunately 433.milc is too large to run
any feasible tests on my test machine, which runs a 4-core Intel(R) Core(TM) i7-4500U CPU
@ 1.80GHz and 8 gigabytes of memory. The Unison test suite will be used instead, which is
a sample of functions from the SPEC2006 benchmarks. There are 23 functions and their names
are listed in \ref{appendix:function_names}. These functions will together make a
\textit{program}. Since they do not make up a complete executable they cannot be linked
nor executed. Instead linking will be simulated by placing them in the same order every time
estimated to ensure a fair comparison between them.

Our method of testing is thus as follows:

\begin{enumerate}
	\item For every function generate 1000 versions
	\item Version 0 of every function will make up program 0, version 1 will make up program
		1 and so forth
	\item For every program find all gadgets and its cost (in cycles)
	\item Calculate our metrics
	\item Repeat for all strategies
		\begin{itemize}
			\item enumerate
			\item registers
			\item schedule
		\end{itemize}
	\item Repeat for all sampling rates
		\begin{itemize}
			\item 1
			\item 10
			\item 100
			\item 1000
		\end{itemize}
\end{enumerate}

In other words, for every combination of strategy and sampling rate (12 in total) 1000
different versions of all 23 functions will be generated. These function versions will
then be combined to yield 1000 different program versions.
