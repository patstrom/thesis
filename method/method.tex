\chapter{Diversification with Unison}


The two strategies explored by \textcite{large-scale-automated} proved to be effective in
the sense that very few gadgets survived between the emitted binaries. Specifically, the
strategies consits of randoimizing the instruction schedule or inserting no-op instructions
randomly in the emitted assembly.

Summarized, the prodecure \textcite{large-scale-automated} used to evaluate the individual
strategies was to generate 25 different versions for each transformation and testing their
performance, frequency of surviving gadgets and file size compared to the original. For
our purposes the instruction schedule transformation is the most interesting, as it relates
more directly to Unison.

The presented result is that randomizing the instruction schedule resulted in an 9\%
slowdown and that it removes on average 95\% of gadgets \textit{with respect to the original executable}.
The file size test for the schedule transformation are described as "inconclusive" but
also that "randomized instruction scheduling has only a negligible effect of file size".
\footnote{More comprehensive tests were done on other transformations, including pairwise
testing of surviving gadgets.}

The problem with the above approach is first and foremost that the amount of generated
binaries is very small. Presumably it will lose effectiveness after some number of
generated binaries, and at some point the pidgeonhole principle will come into effect.
What is not explored is how many different versions can be generated and how different do
they have to be to achieve similar results. In this chapter we aim to explore this.

\section{Gecode Constraint Solver}

Gecode is implemented such that a problem is modeled in a class inheriting from the
\textit{Space} base class. It is in this derived class variables are defined and constraints
are posted. When a solution is found by the Gecode branch and bound engine the virtual
member function \textit{constrain(b)} is called, where \textit{b} is the latest solution.
This function is expected to post constraints on future solutions based on the latest
solution. These constraints accumulate so posting constraints based on the previous solution
is sufficient for diversity.

Each strategy is implemented by constraints that, when satisified, ensures the
generated solutions differ in the correct manner. These constraints are posted in the
\textit{constrain()} function. In general the constraints will make sure that future
solutions does not contain the same combination of variable - value mapping for certain
variables.

Due to the nature of Unison and the Gecode constraint solver calling the implementations
\textit{transformations}, which is the common nomenclature in software diversity, would be
misleading. No code is being explicitly transformed, combinations are instead implicitly
discarded when it is discovered that the constraints cannot be satisfied given a previous
branching.

\input{method/strategies.tex}

\section{Solution Distance}
When exploring combinations with a constraint solver similar solutions are found close to
each-other (in-time).  The number of possible combinations for certain strategies might be
far too large to handle efficiently so when limiting the number of binaries generated it is
beneficial to discard an amount of binaries in between those kept. The contrasting approach
is of course to stop the search after X amount of solutions are found or Y amount of time
has passed, in which case the solutions might be too similar.

\section{Performance}

Unison's main purpose is to generate the most optimal solution. In an effort to achieve
this Unison accepts the basic LLVM-solution as an optional parameter, and posts constraints
to only generate solutions that are \textit{better}. Better in this case is either that
they can be executed in fewer cycles or the size of the binary is smaller. Which is optimized
for is specified as a parameter.

For our purposes, we could limit our diversification strategies to only generate solutions
that execute in fewer (or the same amount of) cycles than the LLVM solution. In other words
we could generate executables with zero overhead \textit{with respect to LLVM's solution}.
Certain strategies would of course have an overhead, but with respect to the \textit{optimal
solution}.

\section{Architecture}

Unison does not currently support the x86 or x86-64 architecture. Only ARM, Hexagon and MIPS
are supported \cite{unison-src}. None of the supported architectures are complex instruction
set architecutres and thus hidden gadgets are not a problem. However, regular gadgets are
still present and for the purposes of systematic strategies for diversity the supported
architectures are sufficient.

\section{Evaluation}

Three key metrics will be evaluated. \textit{Diversity space}, \textit{estimated runtime},
and \textit{surviving gadgets}.

Diversity space and estimated runtime are simple metrics. The diversity space is simply
the total number of solutions found, and the estimated runtime is the issue cycle of the
last scheduled instruction.

Surviving gadgets is chosen as a measure for diversity. This is a natural metric given 
that the schedule strategy is chosen specifically due to being comparable to the work done
by \textcite{large-scale-automated} and that is the measurement they chose. Their test
was constructed as follows:

\begin{itemize}
	\item Generate an executable
	\item Apply transformations to end up with 30 different versions
	\item Compare the transformed executable to the original and count surviving gadgets
		\begin{itemize}
			\item A surviving gadget is a functionally equivalent sequence of instructions of
			length at the same offset.
		\end{itemize}
\end{itemize}

For our purposes the process is modified such that instead of generating one executable
and applying transformation multiple versions will be generated directly and then tested
pairwise.

The program to be evaluated is 433.milc from the SPEC2006 benchmark, as \textcite{large-scale-automated}
found it to be representative of the whole suite. It is fairly large (234 functions) and
thus for feasibility I will evaluate X versions at solution distance Y.
