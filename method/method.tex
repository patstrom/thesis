\chapter{Diversification with Unison}

The two strategies explored by \textcite{large-scale-automated} proved to be effective in
the sense that very few gadgets survived between the emitted binaries. Specifically, the
strategies consists of randomizing the instruction schedule or inserting no-op instructions
randomly in the emitted assembly.

Summarized, the prodecure \textcite{large-scale-automated} used to evaluate the individual
strategies was to generate 25 different versions for each transformation and testing their
performance, frequency of surviving gadgets and file size compared to the original. For
our purposes the instruction schedule transformation is the most interesting, as it relates
more directly to Unison.

The presented result is that randomizing the instruction schedule resulted in an 9\%
slowdown and that it removes on average 95\% of gadgets \textit{with respect to the original executable}.
The file size test for the schedule transformation are described as "inconclusive" but
also that "randomized instruction scheduling has only a negligible effect of file size".
\footnote{More comprehensive tests were done on other transformations, including pairwise
testing of surviving gadgets.}

The problem with the above approach is first and foremost that the amount of generated
binaries is very small. Presumably it will lose effectiveness after some number of
generated binaries, and at some point the pidgeonhole principle will come into effect.
What is not explored is how many different versions can be generated and how different do
they have to be to achieve similar results. In this chapter we aim to explore this.

\section{Gecode Constraint Solver}

% This might need tweaking if restart based search is used.
Gecode is implemented such that a problem is modeled in a class inheriting from the
\textit{Space} base class. It is in this derived class variables are defined and constraints
are posted. When a solution is found by the Gecode branch and bound engine the virtual
member function \textit{constrain(b)} is called, where \textit{b} is the latest solution.
This function is expected to post constraints on future solutions based on the latest
solution. These constraints accumulate so posting constraints based on the previous solution
is sufficient for diversity.

Each strategy is implemented by constraints that, when satisified, ensures the
generated solutions differ in the correct manner. These constraints are posted in the
\textit{constrain()} function. In general the constraints will make sure that future
solutions does not contain the same combination of variable - value mapping for certain
variables.

Due to the nature of Unison and the Gecode constraint solver calling the implementations
\textit{transformations}, which is the common nomenclature in software diversity, would be
misleading. No code is being explicitly transformed, combinations are instead implicitly
discarded when it is discovered that the constraints cannot be satisfied given a previous
branching.

The implementation that makes up the following strategies that are used in the experiment
is henceforth refered to as disUnison (from the word disunity). It is a variation of the
original Unison model, where the key difference is that the behaviour of \textit{constrain()}
is modified. The bulk of the Unison model is still present in the disUnison model.

\input{method/strategies.tex}


\section{Performance}
\label{sec:performance}

An important variable in Unison's original model is cost. It is the deciding factor of
whether a solution is better than another during the branch and bound search. It is a sum
of the estimated cost of each basic block, weighted by the estimated execution frequencies
\cite{unison-docs}. Cost can be either cycles of file size depending on the optimization
goal. For the experiment the optimization goal will be speed, and thus cost will be
measured in cycles.

In the original Unison model the cost variable is used both for branching and during the
branch and bound process. In the disUnison model it is still used for branching, but future
solutions are not bound to have lower cost. When used for branching lower values of cost
are explored before higher ones, and thus lower cost solutions are found before higher
cost solutions.

Unison (and disUnison) accepts the basic LLVM-solution as an optional parameter, and can
post constraints to only generate solutions that are \textit{better}. In other words, we
can generate executables with zero overhead \textit{with respect to LLVM's solution}.
Certain strategies would of course have an overhead, but with respect to the
\textit{optimal solution}. As this would limit the number of possible version this
optional parameter will not be used during the experiment. It is however an exciting
factor to consider for future work.

\section{Sampling Rate}
\label{sec:sampling_rate}

When exploring combinations with a constraint solver similar solutions are found close to
each-other (in-time). A factor in diversification might be to exploit this property
alongside the diversification strategy. Certain strategies might be favourable when
considering execution time but not particularly good at breaking gadgets. However, if
e.g 100 solutions are discarded between every emitted executable perhaps more gadgets are
broken.

An important factor to consider when discarding solutions is that in the disUnison model
branching tries explores lower cost solutions first (see \ref{sec:performance}.
Discarding solutions can thus impact performance in a negative way in the sense that the
later version might have a wider distribution for the cost variable.

Sampling rates of 1, 10, 100 and 1000 will be evaluated for every strategy, where a
sampling rate of 100 means that every 100th solution is kept. Generating 1000 versions
at a sampling rate of 100 would mean that 100000 solutions are explored, 1000 are emitted
and 99000 are discarded.

\section{Number of Possible Versions}

The number of possible combinations is of course not limitless. 1000 version at a sampling
rate of 1000 means that there needs to be at least 1000000 must be found. Unison works
at the function level and for every function there might not be 1000000 possible versions.

Total number of possible combination would be an interesting metric to evaluate, unfortunately
it varies widely between functions and for some it might require days of search. Empirically,
most functions in the suite to be used do have 1000000 versions, so 1000 versions appears
to be a good number of versions to generate.

For those function where the 1000 versions cannot be generated for the given strategy and
sampling rate the ones that have been generated will be re-used so that 1000 programs can
still be generated. More information about these functions can be seen in the
\ref{appendix:function_names}.

\section{Architecture}

Unison does not currently support the x86 or x86-64 architecture. Only ARM, Hexagon and MIPS
are supported \cite{unison-src}. None of the supported architectures are complex instruction
set architecutres and thus hidden gadgets are not a problem. However, regular gadgets are
still present and for the purposes of systematic strategies for diversity the supported
architectures are sufficient. The tests will be conducted on programs compiled for the
Hexagon architecture.

One factor to keep in mind is that the diversification strategies are applied globally.
The strategies are thus supposedly equally effective at destroying hidden gadgets as
regular gadgets.

\section{Experiment}

\subsection{Metrics}

The key metric to be evaluated are surviving gadgets and \textit{cost} (See
\ref{sec:performance}). For the experiment the optimization goal will be speed, and thus
cost will be the estimated execution time of the program (in cycles).

Surviving gadgets will be calculated as the ratio of which each gadget appears in the program
versions. In other words a ratio of 100\% means that the gadget appears in all version
and the strategy was not effective at removing that gadget. Similarly a very low ratio
(close to 0\%) would mean that the strategy was effective as the gadget only appears in
a small number of the programs.

\subsection{Process}

\textcite{large-scale-automated} found 433.milc from the SPEC2006 benchmark suite to be
representative in terms of surviving gadgets. Unfortunately 433.milc is too large to run
any feasible tests on my test machine, which runs a 4-core Intel(R) Core(TM) i7-4500U CPU
@ 1.80GHz and 8 gigabytes of memory. The Unison test suite will be used instead, which is
a sample of functions from the SPEC2006 benchmarks. There are 23 functions and their names
are listed in \ref{appendix:function_names}. These functions will together make a
\textit{program}. Since they are not from the same benchmark they cannot be linked
together. Instead linking will be simulated by placing them in the same order every time.

Our method of testing is thus as follows:

\begin{enumerate}
	\item For every function generate 1000 versions
	\item Version 0 of every function will make up program 0, version 1 will make up program
		1 and so on and so forth
	\item For every program find all gadgets and its cost (in cycles)
	\item Calculate our metrics
	\item Repeat for all strategies
		\begin{itemize}
			\item enumerate
			\item registers
			\item schedule
		\end{itemize}
	\item Repeat for all sampling rates
		\begin{itemize}
			\item 1
			\item 10
			\item 100
			\item 1000
		\end{itemize}
\end{enumerate}
