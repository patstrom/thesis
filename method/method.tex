\chapter{Diversification with Unison}

\section{Automatic Diversity Synthesis with Unison}
\label{sec:unison-model}

In its current state Unison accepts input in the form of the LLVM MIR (see Section
\ref{sec:unison}) of a single function\cite{unison-docs} and outputs LLVM MIR of the same
function with allocated registers and scheduled instructions. It is then to job of
\textit{llc}, the LLVM static compiler to emit architecture specific assembly, which can
be passed through a native assembler and linker to generate an executable\cite{llvm-llc}.

As described in Section \ref{sec:unison} the problem of integrated register allocation and
instruction scheduling in Unison is modeled around \textit{operations} and \textit{operands}.
The problem consists of around 20 variables (see \ref{sec:constraint}) in total so only the
ones relevant for the experiment will be presented. These variables describe how the
operations and their operands relate to the actual instructions, temporaries, registers
and issue cycles.

The only constraint solver currently supported by Unison is Gecode\cite{unison-docs}. A
branch and bound search can be implemented in Gecode by using a branch-and-bound search
engine and overriding the virtual member function \textit{constrain()} of the model. The
\textit{constrain()} function is invoked by the search engine whenever a solution is found
and takes the most recently found solution as an argument. The idea is to post constraints
on the next solution based on the previous solution. These constraints accumulate so
that all future solutions will be affected by all already found solutions.

As mentioned, our intention is to replace the current \textit{contrain()} function of
Unison with one that instead posts constraints to ensure that future solutions are
\textit{different} in some regard.

An important variable in the Unison model is \textit{cost}. It is the deciding factor
of whether a solution is better than another during the branch and bound search. It is a
sum of the estimated cost of each basic block, weighted by the estimated execution
frequencies \cite{unison-docs}. Cost can be either cycles or code size depending on if the
optimization goal is speed or size, respectively. I.e the \textit{constrain()} function of
the Unison model post the constraint that for future combinations to be considered
solutions, in addition to the original constraints, the cost must be less than the cost of
the previously found solution. Given enough execution time, Unison will thus find the
optimal solution with regards to the \textit{cost} variable.


% Do I want to mention branching and such?

\section{disUnison}

The implementation that makes up the following strategies that are used in the experiment
is henceforth referred to as disUnison (from the word disunity). It is a variation of the
original Unison model, where the key difference is that the behaviour of the branch and
bound search is modified. The bulk of the Unison model, that ensures functional code is
emitted, is still present in the disUnison model.

\input{method/strategies.tex}

\section{Sampling Rate}
\label{sec:sampling_rate}

When exploring combinations with a constraint solver similar solutions are found close to
each-other (in-time). A factor in diversification might be to exploit this property
alongside the diversification strategy. Certain strategies might be favorable when
considering execution time but not particularly good at breaking gadgets. However, if
e.g 100 solutions are discarded between every emitted executable perhaps more gadgets are
broken.

In the original Unison model the cost variable is used both for branching and during the
branch and bound process. In the disUnison model it is still used for branching. Lower
cost combinations are explores first. However, future solutions are not bound to have
lower cost. Discarding solutions can thus impact performance in a negative way in the
sense that the later versions might have a higher cost, resulting in a wider cost
distribution across all program versions.

Sampling rates of 1, 10, 100 and 1000 will be evaluated for every strategy, where a
sampling rate of 100 means that every 100th solution is kept. Generating 1000 versions
at a sampling rate of 100 would mean that 100000 solutions are explored, 1000 are emitted
and 99000 are discarded.

The number of possible combinations is of course not limitless. 1000 version at a sampling
rate of 1000 means that there needs to be at least 1000000 possible solutions. Unison works
at the function level and for every function there might not be 1000000 possible versions.

Total number of possible combination would be an interesting metric to evaluate, unfortunately
it varies widely between functions and for some it might require days of search. Empirically,
most functions in the suite to be used do have 1000000 versions, so 1000 versions appears
to be a good number of versions to generate.

For those function where the 1000 versions cannot be generated for the given strategy and
sampling rate the ones that have been generated will be re-used so that 1000 programs can
still be generated. More information about these functions can be seen in appendix
\ref{appendix:function_names}.

\section{Architecture}
\label{sec:arch}

Unison does not currently support the x86 or x86-64 architecture. Only ARM, Hexagon and MIPS
are supported \cite{unison-src}. None of the supported architectures are generally considered
when testing automated software diversity, but for the purposes of evaluating the use of
a systematic approach the supported architectures offers a glimpse at the potential. For
the experiment the code will be compiled for the Hexagon architecture.

Hexagon functions very differently from x86 but for our purposes targeting Hexagon will
still hint at the gadget-breaking potential of the systematic approach. After all,
breaking gadgets is about shifting, adding, removing or otherwise modifying \textit{any}
instruction in the program, and since the diversification strategies are applied globally
they are supposedly equally effective regardless of the placement or structure of the
instruction.

\section{Experimental Protocol}

The goal of the experiment is to provide an brief evaluation of both the code generator as
well as the generated code.

The experiment will be carried out on a computer running a 4-core Intel(R) Core(TM)
i7-4500U CPU @ 1.80GHz and 8 gigabytes of memory.

% TODO: Intro to section
% TODO: Subsection of dataset

\subsection{Metrics}

The metrics to be evaluated are surviving gadgets, both \textit{cost} (See
\ref{sec:unison-model}) metrics and the execution time of the code generator. For the
experiment the optimization goal will be speed, and thus solutions that are estimated to
execute faster is generated first.

% Make a more formal definition of the metric. It is calculated on a population of
% programs and I enumerate gadgets present in the population.
Surviving gadgets will be calculated as the ratio of which each gadget appears in the program
versions. In other words a ratio of 100\% means that the gadget appears in all version
and the strategy was not effective at removing that gadget. Similarly a very low ratio
(close to 0\%) would mean that the strategy was effective as the gadget only appears in
a small number of the programs.

\subsection{Data Set}

The data set to be used is the Unison test suite for Hexagon. In total 23 function will be
used, each of which is from a benchmark in the SPEC2006 suite. These function will together
make up a \textit{program}. Since they do not make up a complete executable they cannot be
linked nor executed. Linking will instead be simulated by placing them in the same order
every time to ensure a fair comparison between strategies and sampling rates.

As mentioned in Section \ref{sec:unison-model} Unison works on the function level, and so
does disUnison. 1000 versions of each function will be generated and labeled from 0 through
999. Version 0 of each function will make up program version 0, version 1 of each
function will make up program version 1 and so forth. I.e one program version consists of
23 functions, and there will be a total of 1000 program versions for each strategy and
sampling rate, yielding a total of 12 programs with 1000 versions each.

\subsection{Process}


Our method of testing is thus as follows:

\begin{enumerate}
	\item For every function generate 1000 versions
	\item Version 0 of every function will make up program 0, version 1 will make up program
		1 and so forth
	\item For every program find all gadgets and its cost (in cycles)
	\item Calculate our metrics
	\item Repeat for all strategies
		\begin{itemize}
			\item enumerate
			\item registers
			\item schedule
		\end{itemize}
	\item Repeat for all sampling rates
		\begin{itemize}
			\item 1
			\item 10
			\item 100
			\item 1000
		\end{itemize}
\end{enumerate}

In other words, for every combination of strategy and sampling rate (12 in total) 1000
different versions of all 23 functions will be generated. These function versions will
then be combined to yield 1000 different program versions.
