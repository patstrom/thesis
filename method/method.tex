\chapter{Diversification with Unison}

In this chapter the relevant parts of the Unison model, the modification to the models that
will be implemented and the experimental protocol will be presented.

The first section, \textit{Automatic Diversity Synthesis with Unison} briefly presents the
Unison model and how to implement our extension of it. The \textit{disUnison} section
describes how we extend the model to provide software diversity, and the
\textit{Experimental Protocol} section describes how the experiment will be conducted.

\section{Automatic Diversity Synthesis with Unison}
\label{sec:unison-model}

In its current state Unison accepts input in the form of the LLVM MIR (see Section
\ref{sec:unison}) of a single function \cite{unison-docs} and outputs LLVM MIR of the same
function with allocated registers and scheduled instructions. It is then the job of
\textit{llc}, the LLVM static compiler to emit architecture specific assembly, which can
be passed through a native assembler and linker to generate an executable \cite{llvm-llc}.

As described in Section \ref{sec:unison} the problem of integrated register allocation and
instruction scheduling in Unison is modeled around \textit{operations} and \textit{operands}.
The problem consists of around 20 variables (see \ref{sec:constraint}) in total so only the
ones relevant for the experiment will be presented. These variables describe how the
operations and their operands relate to the actual instructions, temporaries, registers
and issue cycles.

The only constraint solver currently supported by Unison is Gecode \cite{unison-docs}. A
branch and bound search can be implemented in Gecode by using a branch and bound search
engine and overriding the virtual member function \textit{constrain()} of the model. The
\textit{constrain()} function is invoked by the search engine whenever a solution is found
and takes the most recently found solution as an argument. The idea is to post constraints
on the next solution based on the previous solution. These constraints accumulate so
that all future solutions will be affected by all already found solutions.

As mentioned, our intention is to replace the current \textit{constrain()} function of
Unison with one that instead posts constraints to ensure that future solutions are
\textit{different} in some regard. Remember that in the context of Unison a solution is a
valid instruction schedule and allocation of registers.

\section{Cost}
\label{sec:cost}

An important variable in the Unison model is \textit{cost}. It is the deciding factor
of whether a solution is better than another during the branch and bound search. It is a
sum of the estimated cost of each basic block, weighted by the estimated execution
frequencies \cite{unison-docs}. Cost can be either cycles or code size depending on if the
optimization goal is speed or size, respectively. That is the \textit{constrain()} function
of the Unison model post the constraint that for future combinations to be considered
solutions, in addition to the original constraints, the cost must be less than the cost of
the previously found solution. Given enough execution time, Unison will thus find the
optimal solution with regards to the \textit{cost} variable.

\section{disUnison}
\label{sec:disUnison}

The implementation that makes up the following strategies that are used in the experiment
is henceforth referred to as disUnison (from the word disunity). It is a variation of the
original Unison model, where the key difference is that the behaviour of the branch and
bound search is modified. The bulk of the Unison model, that ensures functional code is
emitted, is still present in the disUnison model.

\input{method/strategies.tex}

\section{Branching Strategy}
\label{sec:branch_strategy}

The disUnison models uses the same branching strategy as the original Unison model. When
the search engine reaches a state where branching is necessary the first decision is to
assign the \textit{cost} (see Section \ref{sec:cost}) variable to its lowest possible
value. If the \textit{cost} variable is already assigned, the branching is done as
follows, in the order listed:

\begin{enumerate}
	\item assign the active operations. (the $a_o$ variable)
	\item assign which instruction should implement each operation.
	\item assign which temporary is connected to each operand. (the $y_p$ variable)
	\item assign which cycle each operation is issued. (the $c_o$ variable)
	\item assign which register is assigned to which temporary. (the $r_t$ variable)
\end{enumerate}

\section{Sampling Rate}
\label{sec:sampling_rate}

When exploring combinations with a constraint solver similar solutions are found close to
each-other (in-time). A factor in diversification might be to exploit this property
alongside the diversification strategy. Certain strategies might be favorable when
considering execution time but not particularly good at breaking gadgets. However, if
e.g 100 solutions are discarded between every emitted executable perhaps more gadgets are
broken.

In the original Unison model the cost variable is used both for branching and during the
branch and bound process. As mentioned in Section \ref{sec:branch_strategy}, in the
disUnison model it is still used for branching. Lower cost combinations are explores
first. However, future solutions are not bound to have lower cost. Discarding solutions
can thus impact performance in a negative way in the sense that the later versions might
have a higher cost, resulting in a wider cost distribution across all program versions.

Sampling rates of 1, 10, 100 and 1000 will be evaluated for every strategy, where a
sampling rate of 100 means that every 100th solution is kept. Generating 1000 versions
at a sampling rate of 100 would mean that 100000 solutions are explored, 1000 are emitted
and 99000 are discarded.

The number of possible combinations is of course not limitless. 1000 version at a sampling
rate of 1000 means that there needs to be at least 1000000 possible solutions. Unison works
at the function level and for every function there might not be 1000000 possible versions.

Total number of possible combination would be an interesting metric to evaluate, unfortunately
it varies widely between functions and for some it might require days of search. Empirically,
most functions in the suite to be used do have 1000000 versions, so 1000 versions appears
to be a good number of versions to generate.

For those function where 1000 versions cannot be generated for the given strategy and
sampling rate the ones that have been generated will be re-used so that 1000 program
versions can still be generated. More information about these functions can be seen in
appendix \ref{appendix:function_names}.

\section{Target Architecture}
\label{sec:arch}

Unison does not currently support the x86 or x86-64 architecture. Only ARM, Hexagon and MIPS
are supported \cite{unison-src}. None of the supported architectures are generally considered
when testing automated software diversity, but for the purposes of evaluating the use of
a systematic approach the supported architectures offers a glimpse at the potential. For
the experiment the code will be compiled for the Hexagon architecture.

Hexagon functions very differently from x86 but for our purposes targeting Hexagon will
still hint at the gadget-breaking potential of the systematic approach. After all,
breaking gadgets is about shifting, adding, removing or otherwise modifying \textit{any}
instruction in the program, and since the diversification strategies (see Section
\ref{sec:disUnison}) are applied globally they are supposedly equally effective regardless
of the placement or structure of the instruction.

\chapter{Experimental Setup}

In order to evaluate both the code generator and the generated code for each strategy a
population of programs for each strategy is required. In this section the data set, the
evaluation metrics and the process for generating the program populations will be
presented.

The experiment will be carried out on a computer running a 4-core Intel(R) Core(TM)
i7-4500U CPU @ 1.80GHz and with 8 gigabytes of memory.

\section{Data Set}

The data set to be used is part of the Unison test suite for the Hexagon architecture. In
total 23 functions will be used, each of which is from a benchmark in the SPEC2006\footnote{https://www.spec.org/cpu2006/ (visited on 21/06/2018)}
suite. These function will together make up a \textit{program}. Since they do not make up
a complete executable they cannot be linked nor executed. Linking will instead be simulated
by placing them in the same order every time to ensure a fair comparison between
strategies and sampling rates.

As mentioned in Section \ref{sec:unison-model} Unison works on the function level, and so
does disUnison. 1000 versions of each function will be generated and labeled from 0 through
999. Version 0 of each function will make up program version 0, version 1 of each function
will make up program version 1 and so forth. That is, one program version consists of 23
functions, and there will be a total of 1000 program versions for each strategy and
sampling rate, yielding a total of 12 programs with 1000 versions each.

\section{Metrics}
\label{sec:metrics}

The metrics to be evaluated are surviving gadgets, \textit{cost} (See \ref{sec:cost}),
both speed and size, and the execution time of the code generator.

The \textit{cost} metric is calculated by a tool called \textit{uni analyze}, which is
part of the Unison toolchain. It accepts the LLVM MIR of a function as input and outputs
the estimated cost for each optimization goal as described in Section \ref{sec:cost}. The
cost in terms of size of a program version will be calculated as the sum of the cost for
each function version that makes up the program version in question (i.e. the total number
of instructions in the program version). The cost in speed of a program version will be
calculated as the  mean between the cost of the corresponding functions.

For the experiment the optimization goal will be speed, and thus solutions that are
estimated to execute faster are generated first. However, both cost in speed and cost in
size will be presented in the results section.

Surviving gadgets will be calculated as the ratio of which each gadget appears among the
population of program versions. All gadgets present in any of the 1000 program versions
will be enumerated. The ratio is calculated as the number of occurrences of every unique
gadget divided by the number of versions, which in this case is 1000. In other words a
ratio of 100\% means that the gadget appears in all version and the strategy was not
effective at breaking that gadget. Similarly a very low ratio (close to 0\%) would mean
that the strategy was effective as the gadget only appears in a small number of the
programs.

\section{Generation Process}

As mentioned in Section \ref{sec:sampling_rate} most functions in the suite has 1000000
different versions so all strategies will be tested for sampling rates up to 1000. There
is a lot of disk I/O involved in generating all versions and even more so when calculating
the metrics. Thus, sampling a population of 1000 is empirically a good choice to keep the
calculations relatively fast.

Our process to generate our test data is as follows:

\begin{enumerate}
	\item For every function generate 1000 versions with "speed" as the optimization goal.
	\item Version 0 of every function will make up program 0, version 1 will make up program
		1 and so forth.
	\item Calculate/record our metrics.
		\begin{itemize}
			\item Execution time of code generator
			\item Cost
				\begin{itemize}
					\item speed
					\item size
				\end{itemize}
			\item Frequency of surviving gadgets
		\end{itemize}
	\item Repeat for all strategies.
		\begin{itemize}
			\item Enumerate.
			\item Registers.
			\item Schedule.
		\end{itemize}
	\item Repeat for all sampling rates.
		\begin{itemize}
			\item 1.
			\item 10.
			\item 100.
			\item 1000.
		\end{itemize}
\end{enumerate}

In other words, for every strategy and sampling 1000 versions of each function will be
generated. Each combination of strategy and sampling rate will thus have 23000 function
versions assosiated with it. Version 0 of every function for each strategy and sampling
rate will make up program version 0 for the corresponding strategy and sampling rate
combination. Version 1 will make up program version 1 and so forth. Each program version
will consist of 23 functions so the process will yield 1000 program versions for each
combination of strategy and sampling rate.

The resulting test data will be 12 programs, one for each combination of strategy and
sampling rate. Each of these 12 programs represents a population of 1000 different versions,
for a total of 12000 program versions. Each version consists of 23 functions for a total
of 276000 function versions. The result will emphasize comparison between the sampling
rates for each strategy as well as the difference between the strategies.
