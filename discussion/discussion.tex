% TODO: Add "global discussion" and threats to validity. Mention here that tests could not
% be run on Homescu et al dataset.

% This should probably be in the discussion.
Unison (and disUnison) accepts the basic LLVM-solution as an optional parameter, and can
post constraints to only generate solutions that are \textit{better}. In other words, we
can generate executables with zero overhead \textit{with respect to LLVM's solution}.
Certain strategies would of course have an overhead, but with respect to the
\textit{optimal solution}. As this would limit the number of possible version this
optional parameter will not be used during the experiment. It is however an exciting
factor to consider for future work.

% Don't talk about what I Don't do here. This should be in the discussion.
\textcite{large-scale-automated} found 433.milc from the SPEC2006 benchmark suite to be
representative in terms of surviving gadgets. Unfortunately 433.milc is too large to run
any feasible tests on my test machine, which runs a 4-core Intel(R) Core(TM) i7-4500U CPU
@ 1.80GHz and 8 gigabytes of memory. The Unison test suite will be used instead, which is
a sample of functions from the SPEC2006 benchmarks. There are 23 functions and their names
are listed in \ref{appendix:function_names}. These functions will together make a
\textit{program}. Since they do not make up a complete executable they cannot be linked
nor executed. Instead linking will be simulated by placing them in the same order every time
estimated to ensure a fair comparison between them.

% TODO: Separate into its on chapter. It seemed fine though.
\chapter{Discussion, Conclusion and Future Work}

The core of the disUnison model, the three strategies, are implemented in just a handful
lines of code and, thanks to being an extension of the Unison model, there is never a risk
of breaking the functionality of the generated code. This is not to say that every
diversification strategy allows for proper executable code to be generated, but thanks to
the nature of constraint solving the result will either be all possible solutions
(given enough execution time) or a proof that no solution, and thus no proper executable,
exists.

Regardless of what the selection of strategies may indicate the possibilities for
diversification are far broader than when approaching the problem in terms of register
allocation and instruction scheduling as separate procedures. It is important to keep in
mind that more unorthodox strategies that exploit the combined approach might be even
more performant. As mentioned in \ref{sec:unison-model} the Unison model consists of more
variables than the 4 explored in this experiment, all of which offers potential for
diversity.

By exploring lower cost solutions first and applying tiny, incremental changes between
solutions the performance of the resulting code is widely distributed but also relatively
good. Only the highest sampling rates for the schedule strategy incurs a significant
overhead. There is also an opportunity to add constraints to the model that steers this
cost in some direction. As mentioned briefly in Section \ref{sec:performance} we could
limit our executable to have a smaller cost than the LLVM solution, or to only incur an
overhead of e.g 5\% compared to the LLVM solution if we want more versions. By not
randomizing we have full control of the process and can limit the resulting code in
whatever way is appropriate.

For a more proper comparison of the systematic approach tests would have to be repeated
targeting the x86-64 architecture. As mentioned in Section \ref{sec:arch} Unison in its
current state does not support the x86-64 architecture. If or when Unison or a similar
tool implements support for the x86-64 architecture, the experiment would have be repeated
on that platform to test whether or not the strategies are equally performant.

Another shortcoming is the difficulty of compiling a large program even without applying
diversification strategies. Most of the functions of 433.milc were too large to find even
one solution (when searching for about one hour). This is an obvious problem for any
practical purpose of the systematic approach. One solution to this problem is to modify
the search heuristics of the disUnison model even further. Unfortunately it would be
difficult, if not impossible, to find an optimal, generally-applicable search strategy.

Using a constraint solver to generate diverse binaries is an attractive approach given
the ease of implementation and the quality of the generated code. The resulting population
of diversified programs shows that the systematic approach has great potential at breaking
gadgets. However, the shortcomings of the experiment and the tool are a testament to the
future work required.
