\chapter{Discussion, Conclusion and Future Work}
\label{chapter:discussion}

% Implementation
The core of the disUnison model, the three strategies, are implemented in just a handful
lines of code and, thanks to being an extension of the Unison model, there is never a risk
of breaking the functionality of the generated code. This is not to say that every
diversification strategy allows for proper executable code to be generated, but thanks to
the nature of constraint solving the result will either be all possible solutions
(given enough execution time) or a proof that no solution, and thus no proper executable,
exists.

Regardless of what the selection of strategies may indicate the possibilities for
diversification are far broader than when approaching the problem in terms of register
allocation and instruction scheduling as separate procedures. It is important to keep in
mind that more unorthodox strategies that exploit the combined approach might be even
more performant. As mentioned in \ref{sec:unison-model} the Unison model consists of more
variables than the 4 explored in this experiment, all of which offers potential for
diversity.

% Cost
By exploring lower cost solutions first and applying tiny, incremental changes between
solutions the performance of the resulting code is widely distributed but also relatively
good. Only the highest sampling rates for the schedule strategy incurs a significant
overhead. There is also an opportunity to add constraints to the model that steers this
cost in some direction. There is opportunity to limit our executables to have a smaller cost
than the LLVM solution, or to only incur an overhead of e.g 5\% compared to the LLVM
solution if we want more versions. By not randomizing we have full control of the process
and can limit the resulting code in whatever way is appropriate.

Unison (and disUnison) accepts the basic LLVM-solution as an optional parameter, and can
post constraints to only generate solutions that are \textit{better}. In other words, we
can generate executables with zero overhead \textit{with respect to LLVM's solution}.
Certain strategies would of course have an overhead, but with respect to the
\textit{optimal solution}. As this would limit the number of possible version this
optional parameter was not used during the experiment. It is however an exciting factor to
consider for future work.

There is a constant trade-off between diversity and execution overhead when generating
diverse populations of executables and the systematic approach is not excluded from this.
From the results we can deduce that there is a correlation that a more diverse population
in terms of gadgets incurs a wider distribution and a higher mean for the cost metric even
for the systematic approach. However, when comparing sampling rates 10 and 100 of the
enumerate and registers strategy the higher sampling rate generates a noticeably more
diverse population, but the mean cost in speed is virtually unchanged and the code size is
only increased by a handful of percentage points. Perhaps a more advanced strategy can
lessen this gap even more and still provide a diverse population of executables.

% Thread to validity
\textcite{large-scale-automated} found 433.milc from the SPEC2006 benchmark suite to be
representative in terms of surviving gadgets. Unfortunately 433.milc was too large for the
experiment and the Unison test suite had to be used instead. Most the functions were too
large to find even one solution (one searching for about one hour). This is an obvious
problem for any practical purpose of the systematic approach. One solution to this problem
is to modify the search heuristics of the disUnison model even further. Unfortunately it
would be difficult, if not impossible, to find an optimal, generally-applicable search
strategy.

For a more proper comparison of the systematic approach tests would have to be repeated for
a more comprehensive data set and target the x86-64 architecture. As mentioned in Section
\ref{sec:arch} Unison in its current state does not support the x86-64 architecture. If or
when Unison or a similar tool implements support for the x86-64 architecture, the
experiment would have be repeated on to test whether or not the strategies are equally
performant.

% Conclusion
Using a constraint solver to generate diverse binaries is an attractive approach given
the ease of implementation and the quality of the generated code. The resulting population
of diversified programs shows that the systematic approach has great potential at breaking
gadgets as well as providing great control of the incurred overhead. However, the
shortcomings of the experiment and the tool are a testament to the future work required.
