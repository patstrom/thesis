\section{LLVM}
LLVM is an umbrella project that provides a collection of tools for developing low-level
toolchains, e.g assemblers, compilers, debuggers, linkers etc. It is designed to be reusable and
applicable to arbitrary programming languages and target architectures. It started as a
research project at the University of Illinois in 2000 and is widely used today by hobbyists
and professionals alike. There are LLVM frontends for languages such as Haskell, Rust,
Swift and Ruby. Clang is also a part of the LLVM project and built upon
the LLVM toolchain. It provides a compiler, debugger and standard library implementation
for the C language family (C, C++, Objective-C, OpenCL, Cuda etc).

% http://www.rubymotion.com/tour/features/
% https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Backends/LLVM
% http://llvm.org/ https://en.wikipedia.org/wiki/Clang


\subsection{Overview of LLVM}

% http://www.aosabook.org/en/llvm.html
LLVM is designed to be modular. A compiler written on top of LLVM will in general consist
of three phases; The front-end, the mid-end (also called optimizer), and the back-end
(also known as code generator). The front-end is responsible for lexical and syntatical
analysis of the source code. The mid-end is responsible for target-indenpendent optimizations
and the back-end handles platform specific tasks such as instruction selection, register
allocation and instruction scheduling. The point of LLVM is that the interface between
these modules are well-defined and thus allow for reuse so that, e.g, a front-end for
C uses the same optimizer as a front-end for Rust. In a similar manner the back-end
for x86 and the back-end for ARM uses the same mid-end. See figure \ref{fig:three_phase_compiler}.
LLVM can thus be seen as a \textit{collection of libraries} that perform, or at least
assists in performing, these different tasks.

\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{background/figures/three_phase_compiler}
	\caption{Three-phase compiler construction. The mid-end is somtimes called an \textit{optimizer}
	and the back-end a \textit{code generator}}
	\label{fig:three_phase_compiler}
\end{figure}

In the case of LLVM, the interface between the front-end and the mid-end is known as the
LLVM IR (\textit{Intermediate Representation}) and is a strongly typed, RISC-like virtual
instruction set that abstracts some details about the machine, such as function call
conventions and registers. To implement a programming language one would then only have to
implement a front-end, that is translate the source code to LLVM IR. One could then pick
and choose from the already existing LLVM optimizer passes and LLVM back-end implementations
to complete the compiler.

Only the back-end is of concern to this thesis, so it will focus exclusively on that.

\subsection{LLVM back-end}

A LLVM back-end, also known as a LLVM code generator, has a well defined input. The
LLVM IR. The output is often an object file, but a backend can also act as an interpreter.
For most of it's task, a LLVM backend uses what is called the \textit{machine IR} to represent
the code during the different stages.

% https://llvm.org/docs/WritingAnLLVMBackend.html
The LLVM back-end is responsible for three main tasks: Intruction selection, register
allocation and instruction scheduling. They are performed mostly in isolation to each
other, which simplifies the architecture but introduces some interesting challenges.

The general behaviour of an LLVM back-end is that it transforms LLVM IR to machine code
for a specific target platform. Exactly what it produces depends on the target architecture
but it is common to generate assembly corresponding to the instruction set of the target
machine.

This paper does not cover instruction selection or other, smaller back-end tasks such as
optimization, ABI implementation, exception handling etc.

\subsubsection{LLVM Machine Representation}

To represent machine specific IR LLVM uses what is called "Machine specific representation".
The machine specific representation consists of target agnostic, in-memory representations
of functions (MachineFuntion), basic blocks (MachineBasicBlock) and instructions (MachinrInstr).
% https://releases.llvm.org/3.8.1/docs/MIRLangRef.html#id8
% https://releases.llvm.org/3.8.1/docs/CodeGenerator.html#machine-code-representation

The machine specific representation can be represented as the LLVM MIR (\textit{Machine
Intermediate Representation}), which is a human readable, YAML serialized format. It is
used to test code generation passes in LLVM. That is, we can stop the LLVM back-end
prematurely and view the current progress in the form of an LLVM MIR.
% https://llvm.org/docs/MIRLangRef.html

For example, the iterative factorial function written in C:

\lstinputlisting[language=C,tabsize=2,frame=single,breaklines=true,showstringspaces=false,
backgroundcolor= \color{lightgray}]
{background/examples/factorial.c}

gets translated to the following MIR when targeting the Hexagon V4 architecture and
terminating after instruction selection, right before register allocation and instruction
scheduling.

\lstinputlisting[tabsize=2,frame=single,breaklines=true,showstringspaces=false,
backgroundcolor= \color{lightgray}]
{background/examples/factorial.mir}

Courtesy of the Unison documentation for the code examples.
% https://github.com/unison-code/unison/tree/master/doc/code

What is interesting to note is that there is an infinite number of virtual registers available,
starting from \%0 and incrementing, as well as some abstract intructions, such as the PHI
function.

\subsubsection{Instruction Scheduling}
% https://llvm.org/docs/CodeGenerator.html#scheduling-and-formation
% https://llvm.org/devmtg/2016-09/slides/Absar-SchedulingInOrder.pdf
Instruction scheduling is the task of choosing how and in which order the selected instructions
will run. In the case of LLVM the scheduling phase is logically seperate from the selection
phase. There exists a wide variety of architecture and they all have different limitations
and constraints.

Perhaps the most intuitive kind of architecture is the in-order processor where all
instructions are executed sequentially and the schedule is staticically generated.

Alternatives to this architecture is and out-of-order processor where all instructions are
fetched and committed in-order, but executed out-of-order. In this case the instructions
are dynamically scheduled by the CPU. You can also have a VLIW (\textit{Very Long Instruction
Word}) processor where multiple instructions can be statically scheduled in parallel.

% http://infolab.stanford.edu/~ullman/dragon/w06/lectures/inst-sched.pdf

As previously mentioned LLVM is a set of libraries, and thus no one way to schedule instructions
in LLVM exists. However, the key factors to take into account during instruction scheduling
is generally to make use of architecture specific features (such as VLIW), avoiding pipeline
stalls by rearranging instructions, reducing register pressure. When scheduling instructions
the main constraint is data-dependencies. Some instructions depends on the result of previous
instructions, and must thus be scheduled sequentially. In the cases where these dependencies
does not exists (or can be broken by e.g introducing new temporaries) the instructions
can be scheduled in a different order or in parallel (if the hardware allows it).

Instruction scheduling can be seen as a problem where the input is a list of resources
(e.g ALUs, FPUs, presence of VLIW) and a execution cycles for each instruction, and the
output is the instruction in the sequence they should be executed for optimal performence
(with regards to whatever factor you are optimizing for). Of course this problem is very
difficult.

\subsubsection{Register Allocation}
% https://llvm.org/pubs/2009-04-SCOPES-RegisterAllocationDeconstructed.pdf
% https://llvm.org/docs/CodeGenerator.html#register-allocator

There are a few different problem concering register allocation. Namely, coalescing, spilling
move insertion and assigment.

Coalescing is the attempt to eliminate move instructions that refers to identical locations,
i.e. remove unnecessary moves. When spilling a register you allocate the variable on the
stack to free up the register for another temporary ("spilling" the temporary to the stack)
and move insertion is used to split the lifetime of temporaries.

In general the task is modeled as a graph-coloring problem. The first step is then to
perform a \textit{liveliness analysis}, where you analyse the temporaries in the program
to determine which values has to co-exist (i.e. be placed in different registers). With
the liveliness analysis in hand a graph can be constructed where each node is a temporary,
edges represents that the two temporaries must be assigned to distinct registers and the
colors are registers. E.g for a machine with 64 registers you would use at most 64 colors
to color the graph. Assigning a temporary to a specific register (perhaps because of
calling conventions) would then be to "pre-color" the corresponding node. Coalescing in
this model would be to "merge" two nodes and spilling would be to remove a node. Spilling
is usually a consequence of not being able to find a coloring of the graph, thus
necessitating that a node be removed.

\subsubsection{Combining Instruction Scheduling and Register Allocation}
% Mention the problem with offset and having to scavenge register
% https://youtu.be/objxlZg01D0?t=48m53s
